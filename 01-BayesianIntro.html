<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Bayesian</title>
    <meta charset="utf-8">
    <meta name="author" content="Hui Lin" />
    <meta name="date" content="2017-09-13" />
    <link href="libs/remark-css-0.0.1/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Bayesian
### Hui Lin
### 2017-09-13

---




class: inverse, center, middle

# Bayesian Basic

---

## Outside view

&gt; _Likelihood function: In statistics, a likelihood function is a function of parameters of a statistical model given data. Likelihood functions play a key role in statistical inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, "likelihood" is often used as a synonym for "probability". In statistics, a distinction is made depending on the roles of outcomes vs. parameters. Probability is used before data are available to describe possible future outcomes given a fixed value for the parameter. Likelihood is used after data are available to describe a function of a parameter for given outcome. [Wikipedia]_

---

## Outside view

- Data have distributions

- Parameters do not

- Distinguish _parameters_ and _statistics_

- Likelihood is not a probability distribution

- Imaginary _population_

- Bayes is sampling theory + priors

- Priors are uniquely subjective

---

## Conceptual friction

- Thinking data must look like likelihood function

- Degrees of freedom

- Sampling as source of all uncertainty (Bootstrap, the magic!?)
    - Estimate any aspect of the distribution of any quantity computed from data `\(\mathbf{Z}=(z_1,\dots, z_N)\)` with `\(z_i=(x_i, y_i)\)`
    - Estimate prediction error: Pr{observation i `\(\in\)` sample b}

&lt;!--
- Defining random effects via sampling design
- Neglect of data uncertainty
--&gt;

---

## Bayesian data analysis

- Contrast with frequentist view
    - Probability is just limiting frequency
    - Uncertainty arises from sampling variation

- Bayesian probability much more general
    - Probability is in the small world
    - Coins are not random, but our ignorance makes them so

- Claim: Bayes easier and more powerful when understood from the inside

- Problem: Many insider views

---

## Small and Large Worlds

&lt;img style="float: right;" src="images/columbo.png" height="350"&gt;

- **Small world**: the world under the model's assumptions. Bayesian models are optimal, in the small world

- **Larger world**: the real world. No guarantee of optimality for any kind of model

- Have to worry about both

---

## Garden of Forking Data

- The future:
    - Full of branching paths
    - Each choice closes some

- The data:
    - Many possible events
    - Each observation eliminates some

---

## Garden of Forking Data

![](images/garden.png)
---

## Garden of Forking Data

![](images/garden1.png)
---

## Garden of Forking Data

![](images/garden2.png)
---

## Garden of Forking Data

![](images/garden3.png)
---

## Garden of Forking Data

![](images/garden4.png)

---

## Garden of Forking Data

![](images/garden5.png)

---

## Garden of Forking Data

![](images/garden6.png)

---

## 

![](images/garden7.png)

---

## 

![](images/garden8.png)

---

## 

![](images/garden9.png)

---

## 

![](images/garden10.png)

---

## Updating

![](images/garden11.png)

???

In this example, the prior data and new data are of the same type: marbles drawn from the bag. But in general, the prior data and new data can be of different types. 

---


## Using Prior Information

- Factory says: blue marble is rare but every bag contains at least one blue and one white


![](images/garden12.png)

???

???
For 1 bag containing [bbbw], they made 2 bags containing[bbww] and 3 bags containing [bwww]. They also ensured that every bag contained at least 1 blue and 1 white.  

---

## Counts to plausibility

- The basis of applied probability:

&gt; Things that can happen more ways are more plausible


![](images/garden13.png)

--

- Plausibility is probability: set of nonnegative real numbers that sum to one

- Probability theory is just a set of shortcuts for counting possibilities

---
class: inverse, center, middle

# Building a model

---
class: center

# Let's Toss A Coin

![](images/garden14.png)

???
Let's go back to the most boring thing statisticians do: toss a coin

---

# Bayesian Model Design Loop

- Data story:  motivate the model by narrating how the data might arise

- Update: educate your model by feeding it the data

- Evaluate: supervision and revision

???
To get the logic moving, we need to make assumptions, and these assumptions constitute the model. Designing a simple Bayesian model benefit from a design loop with three steps.

---

# Data Story

- How do the data arise?

- For  [H T H H H T H T H]:

    - Some real property of the coin: "H" shows up with probability p; "T" with 1-p 
    
    - Each toss independent of each other

- Translate data story into probability statements

---

# Bayesian Updating 

- Bayesian Updating defines optimal learning in small world, converts _prior_ to _posterior_

    - Give your model an information state, before the data: here, we are confident that p is between 0 and 1
    
    - Condition on data to update information state: new confidence in each value of p, conditional on data

---
class: center, middle

&lt;img src="images/BY_Update.png" alt="Drawing" style="height: 600px;"/&gt;

---

# Bayesian Updating 

&lt;img style="float: right;" src="images/BY_Update.png" height="350"&gt;

- Data order irrelevant (model assumption)

      - All-at-once = one-at-a-time = shuffled order
      
- Every posterior is a prior for next observation

- Every prior is posterior of some other inference

.footnote[
[1] R Code: https://github.com/happyrabbit/BayesianIntro
]

---

# Evaluate

- Bayesian inference: Logical answer to question in the form of a model

-  Evaluate:

    - Did the model malfunction?
    
    - Is the answer reasonable?
    
    - Does the question make sense?
    
    - Check sensitivity of answer to changes in assumptions

---

# Model Components

- Given:

    1. Likelihood

    1. Parameters

    1. Prior

- Deduce: Posterior

---

# Likelihood

- Pr(data|assumptions)

    - Defines probability of each observation, conditional "|" on assumptions

    - i.e. relative count of number of ways of seeing data, given a particular conjecture

- In this example, binomial probability:

    `$$Pr(n_H| n, p) = \frac{n!}{n_{H}!(n-n_{H})!} p^{n_{H}} (1-p)^{n-n_{H}}$$`

    ```r
    dbinom(6, size = 9, prob = 0.5)
    ## [1] 0.1640625
    ```
---

# Parameters

- Likelihood contains symbols: `\(n_{H}, n, p\)`

- Some are data: `\((n_{H}, n)\)`

- Others parameters `\((p)\)`

    - Define targets of inference, what is updated

    - These were the conjectures in the bag example

- Which are data and which parameters depend upon your context and questions

---

# Prior

- What do we believe before the data?

- Likelihood &amp; prior define the model's perspective on the data

- Tossing coin example, a uniform (flat) prior

    `$$Pr(p)=\frac{1}{1-0} = 1$$`

--

- Huge literature on choice of prior

- Flat prior is conventional, but hardly ever best choice

    - Always know something (before data) that can improve inference
    
    - Are zero and one plausible values for `\(p\)`? Is `\(p&lt;0.5\)` as plausible as `\(p&gt;0.5\)`?

    - Don't need to get it exactly right; just need to improve
    
---


# Posterior

- Bayesian estimate is always posterior distribution over parameters, Pr(parameter|data)

- Here: `\(Pr(p|n_{H})\)`

- Compute using Bayes' theorem:

    `$$Pr(p|n_H)=\frac{Pr(n_H|P)Pr(p)}{Pr(n_H)}$$`
    `$$Posterior = \frac{Likelihood \times Prior}{Average\ Likelihood}$$`
    
---
class: center, middle

#### Posterior is prior conditioned on evidence 
 
&lt;img src="images/PP.png" alt="Drawing" style="height: 600px;"/&gt;

---

# Computing the posterior

1. Analytical approach (often impossible)

1. Grid approximation (very intensive)

1. Quadratic approximation (approximate)

1. Markov Chain Monte Carlo (intensive)

---

# Grid approximation
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
